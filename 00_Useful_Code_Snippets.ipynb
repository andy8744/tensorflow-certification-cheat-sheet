{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00_Useful-Code-Snippets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVt0mHEELf0hf63sEMuvcw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andy8744/tensorflow-certification-cheat-sheet/blob/main/00_Useful_Code_Snippets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Manipulation"
      ],
      "metadata": {
        "id": "7tvqQYZiuVH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Test split"
      ],
      "metadata": {
        "id": "wZ2lDVtJuf7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)"
      ],
      "metadata": {
        "id": "eaMIzF07uj9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert to One Hot Encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "aOgo_iwouoB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "keras"
      ],
      "metadata": {
        "id": "Afv3GLpw4cHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "y = to_categorical(y)"
      ],
      "metadata": {
        "id": "o1Se1nlhusx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn"
      ],
      "metadata": {
        "id": "Jy8JmOGy4dRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())"
      ],
      "metadata": {
        "id": "mZsSaIrP4Uap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tensorflow"
      ],
      "metadata": {
        "id": "eLC7MVgP4eog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = tf.one_hot(test_encoded_labels, 3).numpy() # neutral, negative, positive"
      ],
      "metadata": {
        "id": "9aheqgAr4UgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pandas"
      ],
      "metadata": {
        "id": "W5uAvnWC4gUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(df)"
      ],
      "metadata": {
        "id": "UnpxcGWN4Uit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pandas Mapping"
      ],
      "metadata": {
        "id": "rbAtIpKLzPU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample.housing.map(dict(yes=1, no=0)) "
      ],
      "metadata": {
        "id": "SXMkf3O4zRJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining features and labels to a tf dataset"
      ],
      "metadata": {
        "id": "SGUTjIKfvJUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "train_labels_dataset = tf.data.Dataset.from_tensor_slices(Y_train)\n",
        "\n",
        "test_features_dataset = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "test_labels_dataset = tf.data.Dataset.from_tensor_slices(Y_test)\n",
        "\n",
        "# Combine labels and features by zipping together -> features, labels\n",
        "train_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset))\n",
        "test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))"
      ],
      "metadata": {
        "id": "KW4HRv_jvFQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorFlow Datasets (TFDS)"
      ],
      "metadata": {
        "id": "FxgfNirgvRMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, test_data), info = tfds.load('imdb_reviews/subwords8k',\n",
        "                                         split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "                                         with_info=True, as_supervised=True)"
      ],
      "metadata": {
        "id": "XZAdlMPXvSwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, test_data), ds_info = tfds.load(name=\"food101\", # target dataset to get from TFDS\n",
        "                                             split=[\"train\", \"validation\"], # what splits of data should we get? note: not all datasets have train, valid, test\n",
        "                                             shuffle_files=True, # shuffle files on download?\n",
        "                                             as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label)\n",
        "                                             with_info=True) # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info)\n",
        "class_names = ds_info.features[\"label\"].names"
      ],
      "metadata": {
        "id": "FuJa70zU0qSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Dataset to Numpy"
      ],
      "metadata": {
        "id": "89LmEezowkyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset2numpy(dataset):\n",
        "  inputs, targets = tuple(zip(*dataset))\n",
        "  inputs = np.array(tf.squeeze(inputs))\n",
        "  targets = np.array(tf.squeeze(targets))\n",
        "  return inputs, targets"
      ],
      "metadata": {
        "id": "iDWG5DdOwlEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalization Layer"
      ],
      "metadata": {
        "id": "AsJyK8PdzV2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
        "normalizer.adapt(X_train.values)"
      ],
      "metadata": {
        "id": "PQA08Yiyzgof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizing for a tf dataset"
      ],
      "metadata": {
        "id": "z0X23uf80GIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm_layer = layers.Normalization()\n",
        "feature_ds = train_dataset.map(lambda x, y: x)\n",
        "norm_layer.adapt(feature_ds)"
      ],
      "metadata": {
        "id": "1Rot2VhO0Fks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Loading"
      ],
      "metadata": {
        "id": "qpLe9suE9Aee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Images with CSV Annotations"
      ],
      "metadata": {
        "id": "cp3GmoUP9GCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = \"./mnist_images_csv/\"\n",
        "df = pd.read_csv(directory + \"train.csv\")\n",
        "\n",
        "file_paths = df[\"file_name\"].values\n",
        "labels = df[\"label\"].values\n",
        "\n",
        "ds_train = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
        "\n",
        "def read_image(image_file, label):\n",
        "  image = tf.io.read_file(directory + image_file)\n",
        "  image = tf.image.decode_image(image, channels=1, dtype=tf.float32)\n",
        "  return image, label\n",
        "\n",
        "ds_train = ds_train.map(read_image).batch(2)"
      ],
      "metadata": {
        "id": "BFNcalb99Ll3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Images in folder with name annotations"
      ],
      "metadata": {
        "id": "pA5PszuE9aPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "directory = \"/content/mnist_images_only\"\n",
        "ds_train = tf.data.Dataset.list_files(str(pathlib.Path(directory+\"/*.jpg\")))\n",
        "#ds_train = tf.data.Dataset.list_files(directory+\"/*.jpg\")\n",
        "\n",
        "def process_path(file_path):\n",
        "  image = tf.io.read_file(file_path)\n",
        "  image = tf.image.decode_jpeg(image, channels=1)\n",
        "  label = tf.strings.split(file_path, \"/\")[3]\n",
        "  label = tf.strings.substr(label, pos=0, len=1)\n",
        "  label = tf.strings.to_number(label, out_type=tf.int64)\n",
        "  return image, label\n",
        "\n",
        "ds_train = ds_train.map(process_path).batch(batch_size)"
      ],
      "metadata": {
        "id": "rNynYSV99dZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Loading with tensorflow"
      ],
      "metadata": {
        "id": "2w_4BFrF9kUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.text_dataset_from_directory(\n",
        "    directory, labels='inferred', label_mode='int',\n",
        "    class_names=None, batch_size=32, max_length=None, shuffle=True, seed=None,\n",
        "    validation_split=None, subset=None, follow_links=False\n",
        ")"
      ],
      "metadata": {
        "id": "z-iDlEV69nHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Visualization"
      ],
      "metadata": {
        "id": "kaFcEP7FyX3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3x3 Matplotlib Subplots"
      ],
      "metadata": {
        "id": "i-RqVp1xvZ09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10)) # specifying the overall grid size\n",
        "\n",
        "for i, image in enumerate(x_new):\n",
        "    plt.subplot(1,3,i+1)    # the number of images in the grid is 5*5 (25)\n",
        "    plt.imshow(image)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sxib49OovfYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot loss and accuracy"
      ],
      "metadata": {
        "id": "1vU7zFIbyfjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(history.history).plot()\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\");"
      ],
      "metadata": {
        "id": "PjbtFE52zB2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix"
      ],
      "metadata": {
        "id": "hA_bO9lYzEer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "def plot_cm(labels, predictions, p=0.5):\n",
        "  cm = confusion_matrix(labels, predictions > p)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "OHIzeu8ez1me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorboard Callback"
      ],
      "metadata": {
        "id": "5cz8-ZnA0Tma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=log_dir\n",
        "  )\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback"
      ],
      "metadata": {
        "id": "kjHFbo6M0k3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Imbalances"
      ],
      "metadata": {
        "id": "5jiPTk5NDk6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examine label imbalance"
      ],
      "metadata": {
        "id": "kzRif4ISDmpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neg, pos = np.bincount(df['Class'])\n",
        "total = neg + pos\n",
        "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
        "    total, pos, 100 * pos / total))"
      ],
      "metadata": {
        "id": "MAIX4sfRDtkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics and early stopping"
      ],
      "metadata": {
        "id": "KmmboaUDD1kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "]\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_prc', \n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='max',\n",
        "    restore_best_weights=True)"
      ],
      "metadata": {
        "id": "ssLSPMHHD4WE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class weights"
      ],
      "metadata": {
        "id": "Chh47AmiD5z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
        "# The sum of the weights of all examples stays the same.\n",
        "weight_for_0 = (1 / neg) * (total / 2.0)\n",
        "weight_for_1 = (1 / pos) * (total / 2.0)\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
        "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
        "\n",
        "# In model.fit: class_weight=class_weightm"
      ],
      "metadata": {
        "id": "aeeMln8SD9S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Oversampling minority class"
      ],
      "metadata": {
        "id": "E9h8IEl9D_Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_res, y_res = ros.fit_resample(train_features, train_labels)"
      ],
      "metadata": {
        "id": "QwnG_kTxEA9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Others"
      ],
      "metadata": {
        "id": "G_cwBwnt0u_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mixed Precision Training"
      ],
      "metadata": {
        "id": "MLvMWwFy3_sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.mixed_precision.set_global_policy(policy=\"mixed_float16\")\n",
        "# Make sure output is set to float32"
      ],
      "metadata": {
        "id": "wkBMcyzh4DOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L2 Regularization"
      ],
      "metadata": {
        "id": "NzONTgOf5pgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "Conv2D(64, 3, padding=\"same\", kernel_regularizer=regularizers.l2(0.01))"
      ],
      "metadata": {
        "id": "Q4WFQZ8G5vUf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}