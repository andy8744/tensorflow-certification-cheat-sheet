{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOZDRW9DnDnnjTTfm+02IRM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andy8744/tensorflow-certification-cheat-sheet/blob/main/02_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EDA"
      ],
      "metadata": {
        "id": "7RizlNtuIHm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "plt.style.use(\"dark_background\")"
      ],
      "metadata": {
        "id": "VWUEbctOOEmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.target.value_counts()"
      ],
      "metadata": {
        "id": "e_gpxqeqIKAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.target.value_counts().plot.bar();"
      ],
      "metadata": {
        "id": "dD-a74YxIN9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.target.describe()"
      ],
      "metadata": {
        "id": "WNpYrli1Jb8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total training samples: {len(train_df)}\")\n",
        "print(f\"Total test samples: {len(test_df)}\")\n",
        "print(f\"Total samples: {len(test_df) + len(train_df)}\")"
      ],
      "metadata": {
        "id": "xGI4e0VlJe67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].values,\n",
        "                                                                            train_df_shuffled[\"target\"].values,\n",
        "                                                                            test_size=0.1,\n",
        "                                                                            random_state=42)"
      ],
      "metadata": {
        "id": "Vic-JBrWIz0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get length of sentences"
      ],
      "metadata": {
        "id": "6VMANaBxJALC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
        "avg_sent_len = np.mean(sent_lens)"
      ],
      "metadata": {
        "id": "SrMVlksUI-IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(sent_lens, bins=100);"
      ],
      "metadata": {
        "id": "Oz02lvUKJFac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How long of a sentence covers 95% of the lengths?\n",
        "output_seq_len = int(np.percentile(sent_lens, 95))\n",
        "output_seq_len"
      ],
      "metadata": {
        "id": "v2wqCe0sJFqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert to one hot encoded (if needed)"
      ],
      "metadata": {
        "id": "Q0n-LNYGIgQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
        "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
        "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())"
      ],
      "metadata": {
        "id": "7JjYp6TZIqXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes, class_names"
      ],
      "metadata": {
        "id": "R1btxiEFIw0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorizer layer"
      ],
      "metadata": {
        "id": "uU6U-qbcJ0Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_length = 10000\n",
        "max_length = 15\n",
        "text_vectorizer = layers.TextVectorization(max_tokens=max_vocab_length, output_sequence_length=max_length)\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "tfyL-iP0JvCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model (LSTM)"
      ],
      "metadata": {
        "id": "Tj8BFfO8KBvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = layers.Embedding(input_dim=max_vocab_length,\n",
        "                              output_dim=128,\n",
        "                              input_length=max_length)\n",
        "\n",
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x = layers.LSTM(64)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "hnrwLXFUJ96F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model (Conv1D)"
      ],
      "metadata": {
        "id": "qo9jY8boKhHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = layers.Embedding(input_dim=max_vocab_length,\n",
        "                              output_dim=128,\n",
        "                              input_length=max_length)\n",
        "\n",
        "model = keras.Sequential([\n",
        "  layers.Input(shape=(1,), dtype=\"string\"),\n",
        "  text_vectorizer,\n",
        "  emedding,\n",
        "  layers.Conv1D(32, 5, activation=\"relu\"),\n",
        "  layers.GlobalMaxPool1D(),\n",
        "  layers.Dropout(0.5),\n",
        "  layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "FTzas8WeKkYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hub Layer w Universal sentence encoder"
      ],
      "metadata": {
        "id": "ZQDK4mndKsBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                           input_shape=[], dtype=tf.string, trainable=False)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    hub_layer,\n",
        "    \n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    \n",
        "    layers.Dense(10, activation=\"relu\"),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    \n",
        "    layers.Dense(1, activation=\"sigmoid\"),\n",
        "])"
      ],
      "metadata": {
        "id": "VaTX1YamK1FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\", input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)"
      ],
      "metadata": {
        "id": "Slpccyu6Yd4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complile and fit"
      ],
      "metadata": {
        "id": "mdfPtcRiK4XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "callbacks = [\n",
        "\tEarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True),\n",
        "\tReduceLROnPlateau(monitor='val_loss', min_lr=1e-7, patience=2, mode='min', verbose=1, factor=0.1),\n",
        "\tModelCheckpoint(monitor='val_loss', filepath='./best_model.h5', save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(train_sentences, train_labels, epochs=30,\n",
        "            validation_data=(val_sentences, val_labels), \n",
        "            callbacks=[callbacks])"
      ],
      "metadata": {
        "id": "54RUNn2JK9Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "-LIegodQLJRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(ytrue, ypred, cm=False):\n",
        "  \"\"\"\n",
        "  Takes as input ground truth and predictions, outputs dictionary of metrics\n",
        "  \"\"\"\n",
        "  accuracy = sklearn.metrics.accuracy_score(ytrue, ypred)\n",
        "  precision, recall, f1, _ = sklearn.metrics.precision_recall_fscore_support(ytrue, ypred, average=\"weighted\")\n",
        "\n",
        "  if cm == True:\n",
        "    confusion_matrix = sklearn.metrics.confusion_matrix(ytrue, ypred)\n",
        "    disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "\n",
        "  return{\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\":f1}\n",
        "\n",
        "model = tf.squeeze(tf.round(model.predict(val_sentences)))\n",
        "model_results = evaluate_model(val_labels, model_preds)\n",
        "\n",
        "model_8_results"
      ],
      "metadata": {
        "id": "wlkzKRx3LLaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text cleaning\n",
        "\n",
        "Need to find a way to import into tensorflow layer"
      ],
      "metadata": {
        "id": "NEZpOLF4L3Bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import neattext as nt\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = nt.TextFrame(text)\n",
        "    text = (text.remove_emails().remove_urls().remove_emojis()\n",
        "            .remove_puncts().remove_stopwords().remove_special_characters()\n",
        "\t\t\t\t\t\t.fix_contractions())\n",
        "    return str(text)\n",
        "\n",
        "train_df[\"text\"] = train_df[\"text\"].apply(lambda x:preprocess_text(x))"
      ],
      "metadata": {
        "id": "Fs9gwD1xL4dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### https://www.tensorflow.org/tutorials/load_data/text"
      ],
      "metadata": {
        "id": "vXJ8ygd2NvsE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}